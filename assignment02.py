# -*- coding: utf-8 -*-
"""Assignment02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o1v3EEIlM5PGSX_w8Pnd85yUfSuJrAj_
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
df=pd.read_csv('/content/Admission_Predict_Ver1.1.csv')
df.head()



print ("DATA INFORMATION AND DATA TYPES")
df.info()

print ("MISSING DATA (IF ANY)")
df.isnull().sum()

df.corr()

plt.figure(figsize=(10,10))
sns.heatmap(df.corr(),annot=True ,cmap='Blues')

plt.subplots(figsize=(12,8))
plt.scatter(df["Chance of Admit "],df["GRE Score"])
plt.xlabel("Chance of Admit")
plt.ylabel("GRE Score")
#Text(0,0.5,'GRE Score')

plt.subplots(figsize=(12,8))
sns.regplot(x="GRE Score", y="Chance of Admit ", data=df)
plt.xlabel("Chance of Admit")
plt.ylabel("GRE Score ")

#Research experience of candidate helps in getting admission
sns.lmplot(x="GRE Score", y="Chance of Admit ", data =df, hue="Research", height=8)

#university ratings
sns.lmplot(x="GRE Score", y="Chance of Admit ", data=df, hue="University Rating", height=8)

admit_high_chance= df[df["Chance of Admit "]>=0.8]
admit_high_chance.info()

admit_high_chance.corr()

plt.subplots(figsize=(12,8))
sns.set_theme(style="darkgrid")
sns.distplot( admit_high_chance["GRE Score"])

#Linear Regression between GRE Scores and the chance of admit:
X= df["GRE Score"].values
#bringing GRE Score in range of 0-1
X=X/340
y= df["Chance of Admit "].values
#sk learn train test split data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25 )
#sk learn linear regression
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
#training the model on training data
lr.fit(X_train.reshape(-1,1),y_train)
y_pred = lr.predict(X_test.reshape(-1,1))
#model score

lr.score(X_test.reshape(-1,1),y_test.reshape(-1,1))

plt.subplots(figsize=(12,8))
plt.scatter(X_train, y_train, color = "red")
plt.plot(X_train, lr.predict(X_train.reshape(-1,1)), color = "green")
plt.title("GRE Score vs Chance of Admit")
plt.xlabel("GRE Score")
plt.ylabel("Chance Of Admit")
plt.show()

#test input
test= 320
val= test/340
val_out=lr.predict(np.array([[val]]))
print("Chance of admission :", val_out[0])

#Creating a model on the entire Data
x = df.drop(['Chance of Admit ','Serial No.'],axis=1)
y = df['Chance of Admit ']
X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state = 7)
#Random Forest regression
from sklearn.ensemble import RandomForestRegressor
regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=5)
regr.fit(X_train,y_train)
regr.score(X_test, y_test)

#let's now work wirh sample data
val = regr.predict([[325,100,3,4.1,3.7,7.67,1]])
print("your chances are in (%) :")
print(val[0]*100)

df.head()
df['Chance of Admit '] =[1 if each > 0.75 else 0 for each in df['Chance of Admit ']]
df.head()

x = df[['GRE Score','TOEFL Score','University Rating','SOP',	'LOR '	,'CGPA',	'Research']]
y = df['Chance of Admit ']
from sklearn.model_selection import train_test_split
print(f"Size of splitted data")
print(f"x_train {X_train.shape}")
print(f"y_train {y_train.shape}")
print(f"x_test {X_test.shape}")
print(f"y_test {y_test.shape}")

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegression
model_dt = DecisionTreeRegressor(random_state = 1)
model_rf = RandomForestRegressor(random_state = 1)
model_lr = LogisticRegression(random_state = 1,solver = 'lbfgs',max_iter =1000)
model_dt.fit(X_train,y_train)
#model_rf.fit(X_train,y_train)
#model_lr.fit(X_train,y_train)

model_rf.fit(X_train,y_train)

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder

# Assuming y_train is your target labels
# Convert continuous labels to binary categories
threshold = 0.75
y_train_binary = np.where(y_train > threshold, 1, 0)

# Initialize and fit the logistic regression model
model_lr = LogisticRegression(random_state=1, solver='lbfgs', max_iter=1000)
model_lr.fit(X_train, y_train_binary)

y_pred_dt = model_dt.predict(X_test)
y_pred_rf = model_rf.predict(X_test)
y_pred_lr = model_lr.predict(X_test)
result = pd.DataFrame({"Actual":y_test, "Predicted" : y_pred_dt})
result

from sklearn import tree
import matplotlib.pyplot as plt
plt.figure(figsize=(30,30))
tree.plot_tree(model_dt, filled=True, fontsize=16)
plt.show()

y_pred_rf = [1 if each > 0.75 else 0 for each in y_pred_rf]

y_test = [1 if each > 0.75 else 0 for each in y_test] #converting the data to binary
from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score
from sklearn.metrics import classification_report
ConfusionMatrixDisplay.from_predictions(y_test,y_pred_rf)

plt.title('Decision Tree')
plt.show()
print(f" Accuracy is {accuracy_score(y_test,y_pred_rf)}")
print(classification_report(y_test,y_pred_rf))

y_pred_lr = model_lr.predict(X_test)

y_pred_lr = [1 if each > 0.75 else 0 for each in y_pred_lr]
ConfusionMatrixDisplay.from_predictions(y_test,y_pred_lr)
plt.title('Logistic Regression')
plt.show()
print(f" Accuracy is {accuracy_score(y_test,y_pred_lr)}")
print(classification_report(y_test,y_pred_lr))

ConfusionMatrixDisplay.from_predictions(y_test,y_pred_rf,xticks_rotation='vertical')
plt.title('Random Forest')
plt.show()
print(f" Accuracy is {accuracy_score(y_test,y_pred_rf)}")
print(classification_report(y_test,y_pred_rf))

